Approach:
    Projection
        Project every training instance perpendicularly onto this subspace.
        But subspace may twist and turn, such as in the famous Swiss roll toy dataset
    Manifold Learning
        Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of the Swiss roll together, as shown on the left of Figure 8-5. However, what you really want is to unroll the Swiss roll to obtain the 2D dataset on the right
    
PCA:
    First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.
    Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.
    
    Principal Components:
        PCA assumes that the dataset is centered around the origin.
    
    It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse
transformation of the PCA projection. Of course this wonâ€™t give you back the original data, since the
projection lost a bit of information (within the 5% variance that was dropped), but it will likely be quite
close to the original data. The mean squared distance between the original data and the reconstructed data
(compressed and then decompressed) is called the reconstruction error

Kernel PCA:
    a linear decision boundary in the high-dimensional feature
space corresponds to a complex nonlinear decision boundary in the original space

    it possible to perform complex nonlinear
projections for dimensionality reduction

    Kernel PCA is often good at
preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a
twisted manifold

Locally Linear Embedding (LLE):
    LLE works by first measuring how each training instance linearly relates to its
closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where
these local relationships are best preserved

Multidimensional Scaling (MDS):
    Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve the distances
    between the instances (see Figure 8-13).

Isomap:    
    Isomap creates a graph by connecting each instance to its nearest neighbors, then reduces
dimensionality while trying to preserve the geodesic distances9 between the instances.

TSNE:
    t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep
similar instances close and dissimilar instances apart. It is mostly used for visualization, in
particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST
images in 2D).

Linear Discriminant Analysis (LDA):    
    Linear Discriminant Analysis (LDA) is actually a classification algorithm, but during training it
    learns the most discriminative axes between the classes, and these axes can then be used to define a
    hyperplane onto which to project the data. The benefit is that the projection will keep classes as far
    apart as possible, so LDA is a good technique to reduce dimensionality before running another
    classification algorithm such as an SVM classifier





