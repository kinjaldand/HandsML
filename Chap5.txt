SVMs:
    Capable of performing linear or nonlinear classification, regression, and even outlier detection
    well suited for classification of complex but small- or medium-sized datasets.
    The decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors.
    SVMs are sensitive to the feature scales, as you can see in Figure 5-2: on the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling  the decision boundary looks much better.
    
Hard Margin Classification:
    If we strictly impose that all instances be off the street and on the right side, this is called hard marginclassification.
    Two Issues:
        it only works if the data is linearly separable, 
        and second it is quite sensitive to outliers

Soft Margin Classification:
    To avoid these issues it is preferable to use a more flexible model. 
    The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side).
    In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter: a smaller C value leads to a wider street but more margin violations.
    If your SVM model is overfitting, you can try regularizing it by reducing C.
    Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.
    
    The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. 
    This is automatic if you scale the data using the StandardScaler. Moreover, make sure you set the loss hyperparameter to "hinge", as it is not the default value. 
    Finally, for better performance you should set the dual hyperparameter to False, unless there are more features than training instances.
    
Kernel Trick:
    SVMs you can apply an almost miraculous mathematical technique called the kernel trick (it is explained in a moment). It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them.
    
    Another technique to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark.
    let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with γ = 0.3.
    The simplest approach is to create a landmark at the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that a training set with m instances and n features gets transformed into a training set with m instances and m features
    (assuming you drop the original features). 
    If your training set is very large, you end up with an equally large number of features.
    Gaussian RBF kernel makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them. 
    
    Increasing gamma makes the bell-shape curve narrower  and as a result each instance’s range of influence is smaller: the
    decision boundary ends up being more irregular, wiggling around individual instances. 
    Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influence, and
    the decision boundary ends up smoother. 
    So γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it, and if it is underfitting, you should increase it (similar to the C
    hyperparameter)
    
SVM Regression:
    The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin
    violations.
    The width of the street is controlled by a hyperparameter ϵ.
    Adding more training instances within the margin does not affect the model’s predictions; thus, the model is said to be ϵ-insensitive.
    
    The LinearSVR class scales linearly with the size of the training set (just like the LinearSVC class), while the SVR class gets much too slow when the training set grows large (just like the SVC class).

