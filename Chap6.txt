Decision Tree:
    Decision Trees are versatile Machine Learning algorithms that can perform both
classification and regression tasks, and even multioutput tasks. They are very powerful algorithms,
capable of fitting complex datasets.
    Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e.,
questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have
more than two children.
    No scaling or centering required
    Gini = 0 means Pure, no variations
    a nonparametric model, not because it does not have any parameters (it often has a lot) but because the
number of parameters is not determined prior to training, so the model structure is free to stick closely to
the data. In contrast, a parametric model such as a linear model has a predetermined number of
parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of
underfitting).
    
    
CART:
    Classification And Regression Tree (CART)
    the algorithm first splits the training set in two
subsets using a single feature k and a threshold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k
and tk? It searches for the pair (k, tk) that produces the purest subsets (weighted by their size).

PARAMS:
    min_samples_split (the minimum number of samples a node must have before it can be
split), min_samples_leaf (the minimum number of samples a leaf node must have),
min_weight_fraction_leaf (same as min_samples_leaf but expressed as a fraction of the total
number of weighted instances), max_leaf_nodes (maximum number of leaf nodes), and max_features
(maximum number of features that are evaluated for splitting at each node). Increasing min_*
hyperparameters or reducing max_* hyperparameters will regularize the model
    
Tree Pruning:
    Other algorithms work by first training the Decision Tree without restrictions, then pruning (deleting) unnecessary nodes. A node
whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant.
Standard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of
chance (which is called the null hypothesis). If this probability, called the p-value, is higher than a given threshold (typically 5%,
controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until
all unnecessary nodes have been pruned.

